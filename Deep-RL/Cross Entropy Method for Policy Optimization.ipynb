{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEM for Policy Optimization. Use OpenAi Gym and Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab1 -> Deep Reinforcement Learning - John Schulman MLSS\n",
    "http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/lab1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 -> Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 -> Define discrete action policy generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeterministicDiscreteActionLinearPolicy(object):\n",
    "    def __init__(self, theta, obs_space, act_space):\n",
    "        obs_dim = obs_space.shape[0]\n",
    "        act_dim = act_space.n\n",
    "        assert len(theta) == obs_dim * act_dim + act_dim\n",
    "        self.W = theta[0 : obs_dim * act_dim].reshape(obs_dim, act_dim)\n",
    "        self.b = theta[obs_dim * act_dim : None]\n",
    "        \n",
    "    def act(self, obs):\n",
    "        aprob = np.dot(obs, self.W) + self.b\n",
    "        act = aprob.argmax()\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 -> Define continuous action policy generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeterministicContinuousActionLinearPolicy(object):\n",
    "    def __init__(self, theta, obs_space, act_space):\n",
    "        obs_dim = obs_space.shape[0]\n",
    "        act_dim = act_space.shape[0]\n",
    "        assert len(theta) == obs_dim * act_dim + act_dim\n",
    "        self.W = theta[0 : obs_dim * act_dim].reshape(obs_dim, act_dim)\n",
    "        self.b = theta[obs_dim * act_dim : None]\n",
    "        self.act_space = act_space\n",
    "        \n",
    "    def act(self, obs):\n",
    "        aprob = np.dot(obs, self.W) + self.b\n",
    "        act = np.clip(aprob, self.act_space.low, self.act_space.high)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 -> Generate appropriate policy using theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_policy(theta):\n",
    "    if isinstance(env.action_space, Discrete):\n",
    "        return DeterministicDiscreteActionLinearPolicy(theta, \n",
    "            env.observation_space, env.action_space)\n",
    "    elif isinstance(env.action_space, Box):\n",
    "        return DeterministicContinuousActionLinearPolicy(theta,\n",
    "            env.observation_space, env.action_space)\n",
    "    else:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 -> Do an episode using policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_episode(policy, render=False):\n",
    "    obs = env.reset()\n",
    "    total_rew = 0\n",
    "    for t in range(num_step):\n",
    "        act = policy.act(obs)\n",
    "        obs, rew, done, _info = env.step(act)\n",
    "        total_rew = total_rew + rew\n",
    "        if render and t % 3 == 0: env.render()\n",
    "        if done: break\n",
    "    return total_rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 -> Evaluate a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_evaluation(theta):\n",
    "    policy = make_policy(theta)\n",
    "    rew = do_episode(policy)\n",
    "    return rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 -> Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_step = 500\n",
    "n_iter = 100\n",
    "batch_size = 25\n",
    "elite_fac = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 -> Initialize game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-14 14:56:57,130] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"CartPole-v0\") # Discrete\n",
    "# env = gym.make(\"Pendulum-v0\") # Continuous\n",
    "# env = gym.make(\"Acrobot-v1\") # Discrete\n",
    "env = gym.make(\"MountainCar-v0\") # Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 -> Initialize mean and std of theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isinstance(env.action_space, Discrete):\n",
    "    theta_dim = (env.observation_space.shape[0] + 1) * env.action_space.n\n",
    "elif isinstance(env.action_space, Box):\n",
    "    theta_dim = (env.observation_space.shape[0] + 1) * env.action_space.shape[0]\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "theta_mean = np.zeros(theta_dim)\n",
    "theta_std = np.ones(theta_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop\n",
    "1. Sample %batch_size% thetas ~ N(theta_mean, theta_std)\n",
    "2. For each theta in thetas, do an episode, get the reward f1, f2, ..., fn\n",
    "3. Get top %elite_fac%% reward, to get top %elite_fac%% theta named elite set\n",
    "4. Fit elite set with N(theta_mean', theta_std') using max(mean, std)(sigma(fi√ólogp(thetai)))\n",
    "5. Update theta_mean and theta_std with theta_mean' and theta_std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1. mean f:     -200. max f:     -200\n",
      "Iteration 2. mean f:     -200. max f:     -200\n",
      "Iteration 3. mean f:     -200. max f:     -200\n",
      "Iteration 4. mean f:     -200. max f:     -200\n",
      "Iteration 5. mean f:     -200. max f:     -200\n",
      "Iteration 6. mean f:     -200. max f:     -200\n",
      "Iteration 7. mean f:     -200. max f:     -200\n",
      "Iteration 8. mean f:     -200. max f:     -200\n",
      "Iteration 9. mean f:     -200. max f:     -200\n",
      "Iteration 10. mean f:     -200. max f:     -200\n",
      "Iteration 11. mean f:     -200. max f:     -200\n",
      "Iteration 12. mean f:     -200. max f:     -200\n",
      "Iteration 13. mean f:     -200. max f:     -200\n",
      "Iteration 14. mean f:     -200. max f:     -200\n",
      "Iteration 15. mean f:     -200. max f:     -200\n",
      "Iteration 16. mean f:     -200. max f:     -200\n",
      "Iteration 17. mean f:     -200. max f:     -200\n",
      "Iteration 18. mean f:     -200. max f:     -200\n",
      "Iteration 19. mean f:     -200. max f:     -200\n",
      "Iteration 20. mean f:     -200. max f:     -200\n",
      "Iteration 21. mean f:     -200. max f:     -200\n",
      "Iteration 22. mean f:     -200. max f:     -200\n",
      "Iteration 23. mean f:     -200. max f:     -200\n",
      "Iteration 24. mean f:     -200. max f:     -200\n",
      "Iteration 25. mean f:     -200. max f:     -200\n",
      "Iteration 26. mean f:     -200. max f:     -200\n",
      "Iteration 27. mean f:     -200. max f:     -200\n",
      "Iteration 28. mean f:     -200. max f:     -200\n",
      "Iteration 29. mean f:     -200. max f:     -200\n",
      "Iteration 30. mean f:     -200. max f:     -200\n",
      "Iteration 31. mean f:     -200. max f:     -200\n",
      "Iteration 32. mean f:     -200. max f:     -200\n",
      "Iteration 33. mean f:     -200. max f:     -200\n",
      "Iteration 34. mean f:     -200. max f:     -200\n",
      "Iteration 35. mean f:     -200. max f:     -200\n",
      "Iteration 36. mean f:     -200. max f:     -200\n",
      "Iteration 37. mean f:     -200. max f:     -200\n",
      "Iteration 38. mean f:     -200. max f:     -200\n",
      "Iteration 39. mean f:     -200. max f:     -200\n",
      "Iteration 40. mean f:     -200. max f:     -200\n",
      "Iteration 41. mean f:     -200. max f:     -200\n",
      "Iteration 42. mean f:     -200. max f:     -200\n",
      "Iteration 43. mean f:     -200. max f:     -200\n",
      "Iteration 44. mean f:     -200. max f:     -200\n",
      "Iteration 45. mean f:     -200. max f:     -200\n",
      "Iteration 46. mean f:     -200. max f:     -200\n",
      "Iteration 47. mean f:     -200. max f:     -200\n",
      "Iteration 48. mean f:     -200. max f:     -200\n",
      "Iteration 49. mean f:     -200. max f:     -200\n",
      "Iteration 50. mean f:     -200. max f:     -200\n",
      "Iteration 51. mean f:     -200. max f:     -200\n",
      "Iteration 52. mean f:     -200. max f:     -200\n",
      "Iteration 53. mean f:     -200. max f:     -200\n",
      "Iteration 54. mean f:     -200. max f:     -200\n",
      "Iteration 55. mean f:     -200. max f:     -200\n",
      "Iteration 56. mean f:     -200. max f:     -200\n",
      "Iteration 57. mean f:     -200. max f:     -200\n",
      "Iteration 58. mean f:     -200. max f:     -200\n",
      "Iteration 59. mean f:     -200. max f:     -200\n",
      "Iteration 60. mean f:     -200. max f:     -200\n",
      "Iteration 61. mean f:     -200. max f:     -200\n",
      "Iteration 62. mean f:     -200. max f:     -200\n",
      "Iteration 63. mean f:     -200. max f:     -200\n",
      "Iteration 64. mean f:     -200. max f:     -200\n",
      "Iteration 65. mean f:     -200. max f:     -200\n",
      "Iteration 66. mean f:     -200. max f:     -200\n",
      "Iteration 67. mean f:     -200. max f:     -200\n",
      "Iteration 68. mean f:     -200. max f:     -200\n",
      "Iteration 69. mean f:     -200. max f:     -200\n",
      "Iteration 70. mean f:     -200. max f:     -200\n",
      "Iteration 71. mean f:     -200. max f:     -200\n",
      "Iteration 72. mean f:     -200. max f:     -200\n",
      "Iteration 73. mean f:     -200. max f:     -200\n",
      "Iteration 74. mean f:     -200. max f:     -200\n",
      "Iteration 75. mean f:     -200. max f:     -200\n",
      "Iteration 76. mean f:     -200. max f:     -200\n",
      "Iteration 77. mean f:     -200. max f:     -200\n",
      "Iteration 78. mean f:     -200. max f:     -200\n",
      "Iteration 79. mean f:     -200. max f:     -200\n",
      "Iteration 80. mean f:     -200. max f:     -200\n",
      "Iteration 81. mean f:     -200. max f:     -200\n",
      "Iteration 82. mean f:     -200. max f:     -200\n",
      "Iteration 83. mean f:     -200. max f:     -200\n",
      "Iteration 84. mean f:     -200. max f:     -200\n",
      "Iteration 85. mean f:     -200. max f:     -200\n",
      "Iteration 86. mean f:     -200. max f:     -200\n",
      "Iteration 87. mean f:     -200. max f:     -200\n",
      "Iteration 88. mean f:     -200. max f:     -200\n",
      "Iteration 89. mean f:     -200. max f:     -200\n",
      "Iteration 90. mean f:     -200. max f:     -200\n",
      "Iteration 91. mean f:     -200. max f:     -200\n",
      "Iteration 92. mean f:     -200. max f:     -200\n",
      "Iteration 93. mean f:     -200. max f:     -200\n",
      "Iteration 94. mean f:     -200. max f:     -200\n",
      "Iteration 95. mean f:     -200. max f:     -200\n",
      "Iteration 96. mean f:     -200. max f:     -200\n",
      "Iteration 97. mean f:     -200. max f:     -200\n",
      "Iteration 98. mean f:     -200. max f:     -200\n",
      "Iteration 99. mean f:     -200. max f:     -200\n",
      "Iteration 100. mean f:     -200. max f:     -200\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iter):\n",
    "    thetas = np.random.multivariate_normal(theta_mean, np.eye(theta_dim) * theta_std, batch_size)\n",
    "    rewards = [noisy_evaluation(theta) for theta in thetas]\n",
    "    n_elite = int(batch_size * elite_fac)\n",
    "    elite_inds = np.argsort(rewards)[batch_size - n_elite : None]\n",
    "    elite_thetas = [thetas[i] for i in elite_inds]\n",
    "    theta_mean = np.sum(np.array([rewards[i] for i in elite_inds]).reshape(-1, 1) * np.array(elite_thetas), axis=0) / np.sum(np.array([rewards[i] for i in elite_inds]))\n",
    "    theta_std = np.sum(np.array([rewards[i] for i in elite_inds]).reshape(-1, 1) * np.square(np.array(elite_thetas) - theta_mean), axis=0) / np.sum(np.array([rewards[i] for i in elite_inds]))\n",
    "    print(\"Iteration %i. mean f: %8.3g. max f: %8.3g\" % (iteration + 1, np.mean(rewards), np.max(rewards)))\n",
    "    # do_episode(make_policy(theta_mean), render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
